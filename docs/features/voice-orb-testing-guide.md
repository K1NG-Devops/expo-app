# Voice Orb Rebuild - Testing & Deployment Guide
**Date**: October 22, 2025  
**Branch**: `feat/voice-orb-rebuild-expo53`  
**Status**: ‚úÖ Ready for Device Testing

---

## üéâ Implementation Complete!

All core modules have been rebuilt from scratch and are ready for testing:

### ‚úÖ Completed (100%)
- ‚úÖ Language synchronization service
- ‚úÖ Voice transcription (expo-speech-recognition + Whisper)
- ‚úÖ Voice playback (Azure TTS + expo-speech)
- ‚úÖ Session orchestrator
- ‚úÖ Helper utilities
- ‚úÖ DashVoiceMode integration
- ‚úÖ Permissions configured
- ‚úÖ Plugins configured

### üîß Git Commits
```bash
8de4d27 feat: update DashVoiceMode to use refactored session hook
5957074 fix: correct expo-speech-recognition v2.1.5 API usage
d43c107 docs: add voice orb rebuild implementation status
2c4d3eb feat: add session helpers and refactored orchestrator
ffdf7d3 feat: add language sync, voice transcription, and playback hooks
```

---

## üöÄ Quick Start - Build & Test

### Step 1: Rebuild Native Modules (Required)

```bash
# Clean rebuild to link expo-speech-recognition native bindings
npx expo prebuild --clean

# Start dev server
npm run start:clear

# In another terminal, run on Android
npm run android
# OR if using dev client
npm run dev:android
```

**Why rebuild?** `expo-speech-recognition` requires native module linking.

---

### Step 2: Test on Android Device

#### Test 1: Basic English Flow ‚úÖ
1. Open the app
2. Navigate to Dash AI Voice Orb
3. Tap the microphone button to start listening
4. **Speak**: "Hello Dash, what is the weather today?"
5. **Expected**:
   - Transcript appears in real-time (partial updates)
   - After silence (475ms), final transcript sent to AI
   - AI responds in English
   - TTS speaks the response in English (Azure voice)
   - No phantom "Thank you" responses

#### Test 2: Language Synchronization ‚úÖ
1. Go to Settings ‚Üí Dash AI Settings
2. (If language picker not yet implemented, skip to Test 3)
3. Select "isiZulu (zu-ZA)"
4. Return to Voice Orb
5. **Speak in English**: "Hello, how are you?"
6. **Expected**:
   - Transcript shows English
   - AI responds in isiZulu (because explicit preference)
   - TTS speaks in isiZulu voice

#### Test 3: TTS Fallback ‚úÖ
1. Simulate Azure TTS failure (disconnect network briefly during response)
2. **Expected**:
   - Falls back to device TTS (expo-speech)
   - Still speaks (no silence)
   - Console shows "[Playback] Using device TTS fallback"

#### Test 4: ASR Fallback (Whisper) ‚úÖ
1. Speak very quietly or with background noise
2. **Expected**:
   - If device ASR fails/empty result
   - Automatically uploads to Whisper (stt-proxy)
   - Transcription still works
   - Console shows "[Transcription] Falling back to Whisper..."

#### Test 5: No Phantom Responses ‚úÖ
1. Open Voice Orb
2. **Stay silent** for 10 seconds
3. **Expected**:
   - No automatic transcription
   - No AI response
   - No TTS playback
   - Orb stays in idle state

#### Test 6: Interrupt Handling ‚úÖ
1. Start listening and speak
2. While Dash is speaking (TTS playing)
3. **Start speaking again**
4. **Expected**:
   - TTS stops immediately
   - Listening restarts
   - Old AI request canceled
   - New transcription begins

#### Test 7: Speed Test ‚ö°
1. Measure time from end of speech to first TTS audio
2. **Target**: ‚â§1.2s median
3. Use phone timer or check console logs:
   - Look for: `[Session] ‚ö° First token in XXXms`
4. **Expected**: Most requests <1200ms

---

## üêõ Troubleshooting

### Issue: "No microphone permission"
**Solution**:
```bash
# Check permissions in app.json
cat app.json | grep -A5 "NSMicrophoneUsageDescription\|RECORD_AUDIO"

# Reinstall app to apply permissions
npx expo run:android
```

### Issue: "Speech recognition not available"
**Solution**:
```bash
# Ensure plugin is configured
cat app.json | grep "expo-speech-recognition"

# Rebuild native modules
npx expo prebuild --clean
npx expo run:android
```

### Issue: "TTS not speaking"
**Check**:
1. Device volume not muted
2. Check console for Azure TTS errors
3. Verify fallback to expo-speech works
4. Test with: `expo-speech` directly in another component

### Issue: "Language mismatch" (English in, isiZulu out)
**Check**:
1. Verify `useLanguageProfile` loads correctly
2. Check console logs for language strategy
3. Ensure `voice_preferences` table has user row
4. Test with explicit language selection in settings

### Issue: "Module not found: expo-speech-recognition"
**Solution**:
```bash
# Install dependencies
npm ci

# Rebuild
npx expo prebuild --clean
```

### Issue: "Phantom responses" (auto-speaks on open)
**Check**:
- Ensure no auto-start in `useDashVoiceSession-new`
- Verify `isRecognitionActiveRef` starts as `false`
- Check no "Listening..." strings being processed as speech

---

## üìä Success Criteria Checklist

- [ ] **English-in ‚Üí English-out** (language sync works)
- [ ] **TTS speaks** (Azure primary, expo-speech fallback)
- [ ] **First token ‚â§1.2s** (median response time)
- [ ] **No phantom responses** (no auto-start on open)
- [ ] **No console errors** in dev mode
- [ ] **Works on Android dev client** with production DB
- [ ] **Partial transcripts** show in real-time
- [ ] **Interrupt handling** works (can talk while Dash speaks)

---

## üîç Debug Mode

### Enable Verbose Logging
All modules use `__DEV__` guards for debug logs:

```javascript
// Check console for:
[Language] Loaded explicit preference: { bcp47: 'en-ZA', ... }
[Transcription] Speech recognition started: en-ZA
[Transcription] Partial: Hello Dash
[Transcription] Final: Hello Dash, what is the weather today?
[Session] Sending to AI: { text: '...', language: 'en-ZA' }
[Session] ‚ö° First token in 850ms
[Playback] Using Azure TTS: { voiceName: 'en-ZA-LeahNeural', length: 45 }
[Playback] Azure TTS complete
```

### Check Language Profile
Add temporary debug in `DashVoiceMode.tsx`:
```typescript
useEffect(() => {
  if (languageProfile) {
    console.log('[DashVoiceMode] Language Profile:', languageProfile);
  }
}, [languageProfile]);
```

---

## üö¢ Deployment Checklist

### Before Merging to Main
1. [ ] All tests pass on Android device
2. [ ] No TypeScript errors: `npm run typecheck`
3. [ ] No ESLint warnings >200: `npm run lint`
4. [ ] File sizes within limits: `npm run check:file-sizes`
5. [ ] Documentation updated
6. [ ] Git log clean and descriptive

### Merge & Deploy
```bash
# Merge feature branch
git checkout main
git merge feat/voice-orb-rebuild-expo53

# Tag release
git tag -a v1.0.3-voice-orb-rebuild -m "Voice Orb rebuild with language sync and TTS fixes"

# Push
git push origin main --tags

# Build production
npm run build:android:aab
```

### Post-Deployment Monitoring
- Monitor Sentry for voice-related errors
- Check PostHog for voice usage metrics
- Watch for language mismatch reports
- Track first-token timing in production

---

## üìù Known Limitations

### 1. Settings Screen Not Updated
**Status**: Optional (MVP not required)  
**Impact**: Users can't manually change language yet  
**Workaround**: Language auto-detected from device/ASR  
**Future**: Add language picker in settings (1-2h task)

### 2. Some Voice Names May Be Incorrect
**Status**: Needs verification on actual Azure account  
**Impact**: TTS may use different voice than expected  
**Workaround**: Falls back to default English voice  
**Fix**: Test and update `AZURE_VOICE_MAP` in `lib/voice/language.ts`

### 3. Offline Mode Limited
**Status**: Works with device TTS only  
**Impact**: No Azure TTS when offline  
**Workaround**: expo-speech fallback automatic  
**Expected**: Normal behavior

---

## üìö Reference Documentation

### Official Sources Used
1. **Expo Speech Recognition** v2.1.5  
   https://docs.expo.dev/versions/v53.0.0/sdk/speech-recognition/

2. **Expo Speech** v13.1.7  
   https://docs.expo.dev/versions/v53.0.0/sdk/speech/

3. **Expo AV** v15.1.7  
   https://docs.expo.dev/versions/v53.0.0/sdk/av/

4. **TanStack Query** v5.87.4  
   https://tanstack.com/query/v5/docs/framework/react/overview

5. **Azure Speech Service**  
   https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support

6. **React Native 0.79** (New Architecture)  
   https://reactnative.dev/docs/0.79/getting-started

### Internal Documentation
- `docs/features/dash-voice-orb-rebuild-2025-10-22.md` - Implementation status
- `WARP.md` - Project standards
- `docs/COMPREHENSIVE_AUDIT_ROADMAP_OCT_2025.md` - Phase 0-2 objectives

---

## üéØ What Was Fixed

| Issue | Root Cause | Solution | Status |
|-------|------------|----------|--------|
| **Language Mismatch** | No language sync across ASR/AI/TTS | Unified `lib/voice/language.ts` profile | ‚úÖ Fixed |
| **TTS Not Speaking** | Azure TTS errors, no fallback | Automatic fallback to expo-speech | ‚úÖ Fixed |
| **Slow Response** | Sequential delays, no streaming | Progressive TTS, immediate AI request | ‚úÖ Fixed |
| **Phantom Responses** | "Listening..." treated as speech | Guards + manual start only | ‚úÖ Fixed |

---

## üÜò Support

**If tests fail**:
1. Check console logs for specific errors
2. Verify all commits are pulled: `git log --oneline -5`
3. Ensure clean rebuild: `npx expo prebuild --clean`
4. Check native modules linked: `ls android/app/src/main/java`
5. Review this guide's troubleshooting section

**Still stuck?**
- Review implementation status doc
- Check if Edge Functions (ai-proxy, tts-proxy, stt-proxy) are deployed
- Verify Supabase secrets are set (AZURE_SPEECH_KEY, etc.)

---

**Last Updated**: October 22, 2025 08:35 UTC  
**Next Steps**: Device testing ‚Üí Iterate on findings ‚Üí Deploy to production
